{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "84fe51b2",
            "metadata": {},
            "source": [
                "## 1. Setup & Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f2c8ab40",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
                "!pip install roboflow pillow matplotlib seaborn tqdm numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c66c7688",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone the git repository with the code\n",
                "import os\n",
                "if not os.path.exists('Dice-Detection'):\n",
                "    !git clone https://github.com/Adr44mo/Dice-Detection.git\n",
                "    %cd Dice-Detection\n",
                "else:\n",
                "    %cd Dice-Detection\n",
                "\n",
                "# Add src to path\n",
                "import sys\n",
                "sys.path.append('./src')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4b96cb5c",
            "metadata": {},
            "source": [
                "## 2. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0a65fbb0",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.utils.data as data\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "import json\n",
                "import os\n",
                "\n",
                "# Import custom modules\n",
                "from src.dataset import DiceDetectionDataset, collate_fn\n",
                "from src.model import get_fasterrcnn_model, save_model_checkpoint, get_fasterrcnn_mobilenet\n",
                "from src.training import train_one_epoch, evaluate, get_optimizer, get_lr_scheduler\n",
                "from src.metrics import evaluate_map, print_metrics\n",
                "from src.augmentation import ClassAwareSampler, MosaicAugmentation, apply_random_augmentations\n",
                "from src.visualization import (\n",
                "    plot_class_distribution,\n",
                "    plot_training_history,\n",
                "    display_sample_batch,\n",
                "    visualize_predictions,\n",
                "    plot_ap_comparison\n",
                ")\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "conf_md",
            "metadata": {},
            "source": [
                "## 3. Augmentation Configuration\n",
                "\n",
                "Use the flags below to toggle different augmentation techniques."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "conf_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "# AUGMENTATION CONFIGURATION\n",
                "# -------------------------\n",
                "# Set these flags to enable/disable specific techniques\n",
                "\n",
                "# 1. Class-Aware Sampling\n",
                "# Balances training by oversampling minority classes\n",
                "USE_CLASS_AWARE_SAMPLING = True\n",
                "\n",
                "# 2. Mosaic Augmentation\n",
                "# Combines 4 images into a 2x2 grid\n",
                "USE_MOSAIC_AUGMENTATION = True\n",
                "MOSAIC_PROB = 0.5\n",
                "\n",
                "# 3. Random Augmentations\n",
                "# Applies random flips, color jitter, etc.\n",
                "USE_RANDOM_AUGMENTATION = True\n",
                "\n",
                "print(\"Training Configuration:\")\n",
                "print(f\"  Class-Aware Sampling: {'ENABLED' if USE_CLASS_AWARE_SAMPLING else 'DISABLED'}\")\n",
                "print(f\"  Mosaic Augmentation:  {'ENABLED' if USE_MOSAIC_AUGMENTATION else 'DISABLED'}\")\n",
                "print(f\"  Random Augmentations: {'ENABLED' if USE_RANDOM_AUGMENTATION else 'DISABLED'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b32061ba",
            "metadata": {},
            "source": [
                "## 4. Download Dataset from Roboflow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "745c3ed7",
            "metadata": {},
            "outputs": [],
            "source": [
                "from roboflow import Roboflow\n",
                "\n",
                "# TODO: Add your Roboflow API key\n",
                "rf = Roboflow(api_key=\"YOUR_API_KEY\")\n",
                "\n",
                "# Download the dice dataset\n",
                "project = rf.workspace(\"workspace-spezm\").project(\"dice-0sexk\")\n",
                "dataset = project.version(2).download(\"coco\")\n",
                "\n",
                "print(f\"Dataset downloaded to: {dataset.location}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "08c9a222",
            "metadata": {},
            "source": [
                "## 5. Prepare Datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7595a837",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set dataset paths\n",
                "DATASET_PATH = dataset.location\n",
                "TRAIN_PATH = os.path.join(DATASET_PATH, \"train\")\n",
                "VAL_PATH = os.path.join(DATASET_PATH, \"valid\")\n",
                "TEST_PATH = os.path.join(DATASET_PATH, \"test\")\n",
                "\n",
                "# Check if test set exists\n",
                "has_test_set = os.path.exists(TEST_PATH) and os.path.exists(os.path.join(TEST_PATH, \"_annotations.coco.json\"))\n",
                "print(f\"Test set available: {has_test_set}\")\n",
                "\n",
                "# Create datasets\n",
                "train_dataset = DiceDetectionDataset(\n",
                "    root_dir=TRAIN_PATH,\n",
                "    annotation_file=\"_annotations.coco.json\",\n",
                "    split=\"train\"\n",
                ")\n",
                "\n",
                "val_dataset = DiceDetectionDataset(\n",
                "    root_dir=VAL_PATH,\n",
                "    annotation_file=\"_annotations.coco.json\",\n",
                "    split=\"val\"\n",
                ")\n",
                "\n",
                "# Create test dataset if available\n",
                "if has_test_set:\n",
                "    test_dataset = DiceDetectionDataset(\n",
                "        root_dir=TEST_PATH,\n",
                "        annotation_file=\"_annotations.coco.json\",\n",
                "        split=\"test\"\n",
                "    )\n",
                "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
                "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
                "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
                "else:\n",
                "    test_dataset = None\n",
                "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
                "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
                "    print(f\"No test set - will use validation set for final evaluation\")\n",
                "\n",
                "print(f\"Number of classes: {train_dataset.num_classes}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c4fda747",
            "metadata": {},
            "source": [
                "## 6. Flexible Augmentation Wrapper"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "890e3167",
            "metadata": {},
            "outputs": [],
            "source": [
                "class AugmentedDataset(torch.utils.data.Dataset):\n",
                "    \"\"\"Wrapper dataset that applies configured augmentations\"\"\"\n",
                "    \n",
                "    def __init__(self, base_dataset, use_mosaic=True, mosaic_prob=0.5, use_random=True):\n",
                "        self.base_dataset = base_dataset\n",
                "        self.use_mosaic = use_mosaic\n",
                "        self.use_random = use_random\n",
                "        \n",
                "        if use_mosaic:\n",
                "            self.mosaic = MosaicAugmentation(\n",
                "                base_dataset,\n",
                "                output_size=(640, 640),\n",
                "                prob=mosaic_prob\n",
                "            )\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.base_dataset)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        # Apply Mosaic if enabled\n",
                "        if self.use_mosaic:\n",
                "            image, target = self.mosaic(idx)\n",
                "        else:\n",
                "            image, target = self.base_dataset[idx]\n",
                "        \n",
                "        # Convert tensor to PIL if needed for augmentations\n",
                "        if isinstance(image, torch.Tensor):\n",
                "            from torchvision.transforms import functional as F\n",
                "            image = F.to_pil_image(image)\n",
                "        \n",
                "        # Apply Random Augmentations if enabled\n",
                "        if self.use_random:\n",
                "            image, target = apply_random_augmentations(image, target)\n",
                "        \n",
                "        # Convert back to tensor\n",
                "        from torchvision.transforms import ToTensor\n",
                "        image = ToTensor()(image)\n",
                "        \n",
                "        return image, target\n",
                "\n",
                "# Create augmented training dataset based on config\n",
                "augmented_train_dataset = AugmentedDataset(\n",
                "    train_dataset,\n",
                "    use_mosaic=USE_MOSAIC_AUGMENTATION,\n",
                "    mosaic_prob=MOSAIC_PROB,\n",
                "    use_random=USE_RANDOM_AUGMENTATION\n",
                ")\n",
                "\n",
                "print(f\"Created dataset wrapper with:\")\n",
                "print(f\"  Mosaic: {USE_MOSAIC_AUGMENTATION}\")\n",
                "print(f\"  Random Augmentations: {USE_RANDOM_AUGMENTATION}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c52886bc",
            "metadata": {},
            "source": [
                "## 7. Create Data Loaders with Optional Sampling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "29e98201",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "BATCH_SIZE = 4\n",
                "NUM_WORKERS = 2\n",
                "NUM_EPOCHS = 10\n",
                "LEARNING_RATE = 0.005\n",
                "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
                "\n",
                "# Configure Sampler\n",
                "if USE_CLASS_AWARE_SAMPLING:\n",
                "    train_sampler = ClassAwareSampler(\n",
                "        train_dataset,\n",
                "        samples_per_epoch=len(train_dataset) * 2,  # Sample more to see each class\n",
                "        balance_by='dice_value'\n",
                "    )\n",
                "    shuffle = False  # Don't shuffle when using custom sampler\n",
                "    print(f\"Using Class-Aware Sampler ({len(train_sampler)} samples/epoch)\")\n",
                "else:\n",
                "    train_sampler = None\n",
                "    shuffle = True\n",
                "    print(\"Using Standard Random Sampling\")\n",
                "\n",
                "# Create data loaders\n",
                "train_loader = data.DataLoader(\n",
                "    augmented_train_dataset,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    shuffle=shuffle,\n",
                "    sampler=train_sampler,\n",
                "    num_workers=NUM_WORKERS,\n",
                "    collate_fn=collate_fn\n",
                ")\n",
                "\n",
                "val_loader = data.DataLoader(\n",
                "    val_dataset,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    shuffle=False,\n",
                "    num_workers=NUM_WORKERS,\n",
                "    collate_fn=collate_fn\n",
                ")\n",
                "\n",
                "print(f\"Training batches: {len(train_loader)}\")\n",
                "print(f\"Validation batches: {len(val_loader)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5abe9976",
            "metadata": {},
            "source": [
                "## 8. Initialize Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fb15e5e7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create model\n",
                "model = get_fasterrcnn_mobilenet(\n",
                "    num_classes=train_dataset.num_classes,\n",
                "    pretrained=True,\n",
                "    trainable_backbone_layers=3\n",
                ")\n",
                "\n",
                "model.to(DEVICE)\n",
                "\n",
                "# Setup optimizer and scheduler\n",
                "optimizer = get_optimizer(model, lr=LEARNING_RATE)\n",
                "lr_scheduler = get_lr_scheduler(optimizer, step_size=3, gamma=0.1)\n",
                "\n",
                "print(f\"Model initialized on {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "34919111",
            "metadata": {},
            "source": [
                "## 9. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d024ddbf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training history\n",
                "history = {\n",
                "    'train_loss': [],\n",
                "    'val_loss': [],\n",
                "    'learning_rate': []\n",
                "}\n",
                "\n",
                "best_val_loss = float('inf')\n",
                "CHECKPOINT_DIR = \"checkpoints_comparison\"\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
                "\n",
                "print(\"Starting training...\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "67dd54db",
            "metadata": {},
            "outputs": [],
            "source": [
                "for epoch in range(NUM_EPOCHS):\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    # Train\n",
                "    train_metrics = train_one_epoch(\n",
                "        model, optimizer, train_loader, DEVICE, epoch + 1\n",
                "    )\n",
                "    \n",
                "    # Evaluate\n",
                "    val_metrics = evaluate(model, val_loader, DEVICE)\n",
                "    \n",
                "    # Update learning rate\n",
                "    lr_scheduler.step()\n",
                "    \n",
                "    # Record history\n",
                "    history['train_loss'].append(train_metrics['loss'])\n",
                "    history['val_loss'].append(val_metrics['val_loss'])\n",
                "    history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
                "    \n",
                "    # Print summary\n",
                "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
                "    print(f\"  Train Loss: {train_metrics['loss']:.4f}\")\n",
                "    print(f\"  Val Loss: {val_metrics['val_loss']:.4f}\")\n",
                "    print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
                "    print(f\"  Time: {train_metrics['time']:.2f}s\")\n",
                "    \n",
                "    # Generate config string for filenames\n",
                "    config_str = f\"ca{int(USE_CLASS_AWARE_SAMPLING)}_mo{int(USE_MOSAIC_AUGMENTATION)}_rn{int(USE_RANDOM_AUGMENTATION)}\"\n",
                "    \n",
                "    # Save best model\n",
                "    if val_metrics['val_loss'] < best_val_loss:\n",
                "        best_val_loss = val_metrics['val_loss']\n",
                "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"best_model_{config_str}.pth\")\n",
                "        save_model_checkpoint(\n",
                "            model, optimizer, epoch + 1, val_metrics['val_loss'],\n",
                "            checkpoint_path,\n",
                "            additional_info={\n",
                "                'train_loss': train_metrics['loss'],\n",
                "                'config': config_str\n",
                "            }\n",
                "        )\n",
                "        print(f\"  âœ“ New best model saved!\")\n",
                "    \n",
                "    # Save latest checkpoint\n",
                "    latest_path = os.path.join(CHECKPOINT_DIR, f\"latest_model_{config_str}.pth\")\n",
                "    save_model_checkpoint(\n",
                "        model, optimizer, epoch + 1, val_metrics['val_loss'],\n",
                "        latest_path\n",
                "    )\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Training completed!\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "10_plot",
            "metadata": {},
            "source": [
                "## 10. Plot Training History"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "plot_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_training_history({\n",
                "    'Training Loss': history['train_loss'],\n",
                "    'Validation Loss': history['val_loss'],\n",
                "    'Learning Rate': history['learning_rate']\n",
                "})"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "122963f7",
            "metadata": {},
            "source": [
                "## 11. Final Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f58b0d30",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare evaluation dataset and loader\n",
                "eval_dataset = test_dataset if has_test_set else val_dataset\n",
                "eval_loader = data.DataLoader(\n",
                "    eval_dataset,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    shuffle=False,\n",
                "    num_workers=NUM_WORKERS,\n",
                "    collate_fn=collate_fn\n",
                ")\n",
                "\n",
                "print(f\"Evaluating on: {'Test' if has_test_set else 'Validation'} set\")\n",
                "\n",
                "# Load best model for this config\n",
                "config_str = f\"ca{int(USE_CLASS_AWARE_SAMPLING)}_mo{int(USE_MOSAIC_AUGMENTATION)}_rn{int(USE_RANDOM_AUGMENTATION)}\"\n",
                "best_checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"best_model_{config_str}.pth\")\n",
                "best_checkpoint = torch.load(best_checkpoint_path)\n",
                "model.load_state_dict(best_checkpoint['model_state_dict'])\n",
                "model.to(DEVICE)\n",
                "\n",
                "print(f\"Loaded best model from epoch {best_checkpoint['epoch']}\")\n",
                "\n",
                "# Evaluate\n",
                "detailed_results = evaluate_map(\n",
                "    model, eval_loader, DEVICE,\n",
                "    iou_threshold=0.5,\n",
                "    confidence_threshold=0.05\n",
                ")\n",
                "\n",
                "print(\"\\nDetailed Results at IoU=0.5:\")\n",
                "print_metrics(detailed_results, class_names=eval_dataset.categories)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "58fbd3bb",
            "metadata": {},
            "source": [
                "## 12. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d76e73d5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save evaluation results with unique filename\n",
                "config_str = f\"ca{int(USE_CLASS_AWARE_SAMPLING)}_mo{int(USE_MOSAIC_AUGMENTATION)}_rn{int(USE_RANDOM_AUGMENTATION)}\"\n",
                "results_file = f\"results_{config_str}.json\"\n",
                "\n",
                "results = {\n",
                "    'model': 'Faster R-CNN ResNet50-FPN',\n",
                "    'config': {\n",
                "        'class_aware_sampling': USE_CLASS_AWARE_SAMPLING,\n",
                "        'mosaic_augmentation': USE_MOSAIC_AUGMENTATION,\n",
                "        'random_augmentation': USE_RANDOM_AUGMENTATION,\n",
                "        'mosaic_prob': MOSAIC_PROB\n",
                "    },\n",
                "    'evaluated_on': 'test' if has_test_set else 'validation',\n",
                "    'num_epochs': NUM_EPOCHS,\n",
                "    'batch_size': BATCH_SIZE,\n",
                "    'best_val_loss': float(best_val_loss),\n",
                "    'final_train_loss': float(history['train_loss'][-1]),\n",
                "    'mAP@0.5': float(detailed_results['mAP']),\n",
                "    'per_class_ap': {k: float(v) for k, v in detailed_results.items() if k.startswith('AP_class_')},\n",
                "    'training_history': history\n",
                "}\n",
                "\n",
                "with open(results_file, 'w') as f:\n",
                "    json.dump(results, f, indent=2)\n",
                "\n",
                "print(f\"Results saved to {results_file}\")\n",
                "print(f\"You can compare different runs by changing the flags at the top and re-running.\")"
            ]
        }
    ],
    "metadata": {
        "init_cell": true,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}