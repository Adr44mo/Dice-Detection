{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro"
            },
            "source": [
                "# ðŸŽ² GAN Data Augmentation (COCO Format)\n",
                "\n",
                "This notebook trains a **Conditional DCGAN** and generates synthetic full-scene images with COCO annotations.\n",
                "The output is directly compatible with `DiceDetectionDataset` used in `3_augmentation_comparison.ipynb`.\n",
                "\n",
                "**Output format:**\n",
                "- Full scene images with dice placed on backgrounds\n",
                "- COCO-format `_annotations.coco.json` file"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup"
            },
            "source": [
                "## 1. Setup & Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install"
            },
            "outputs": [],
            "source": [
                "!pip install roboflow torchvision matplotlib tqdm pillow --quiet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "imports"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import random\n",
                "import numpy as np\n",
                "from collections import Counter\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "from tqdm import tqdm\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import torchvision.transforms as transforms\n",
                "import torchvision.utils as vutils\n",
                "\n",
                "SEED = 42\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "data_prep"
            },
            "source": [
                "## 2. Download Dataset & Extract Dice Crops"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "download"
            },
            "outputs": [],
            "source": [
                "from roboflow import Roboflow\n",
                "\n",
                "rf = Roboflow(api_key=\"kd9lS9tvh5StEQtSA6i9\")\n",
                "project = rf.workspace(\"workspace-spezm\").project(\"dice-0sexk\")\n",
                "dataset = project.version(2).download(\"coco\")\n",
                "print(f\"Dataset downloaded to: {dataset.location}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "crop"
            },
            "outputs": [],
            "source": [
                "IMG_SIZE = 64\n",
                "OUTPUT_DIR = 'gan_training_data'\n",
                "ANNOTATION_FILE = f'{dataset.location}/train/_annotations.coco.json'\n",
                "IMAGE_BASE_PATH = f'{dataset.location}/train'\n",
                "\n",
                "with open(ANNOTATION_FILE, 'r') as f:\n",
                "    annotations = json.load(f)\n",
                "\n",
                "categories = {cat['id']: cat['name'] for cat in annotations['categories']}\n",
                "valid_categories = {k: v for k, v in categories.items() if v.isdigit()}\n",
                "print(f\"Categories: {valid_categories}\")\n",
                "\n",
                "image_id_to_info = {\n",
                "    img['id']: {'file_name': img['file_name'], 'width': img['width'], 'height': img['height']}\n",
                "    for img in annotations['images']\n",
                "}\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "for cat_name in valid_categories.values():\n",
                "    os.makedirs(os.path.join(OUTPUT_DIR, cat_name), exist_ok=True)\n",
                "\n",
                "class_counts = Counter()\n",
                "print(\"\\nCropping dice images...\")\n",
                "for ann in tqdm(annotations['annotations']):\n",
                "    category_id = ann['category_id']\n",
                "    if category_id not in valid_categories:\n",
                "        continue\n",
                "    \n",
                "    image_id = ann['image_id']\n",
                "    bbox = ann['bbox']\n",
                "    category_name = valid_categories[category_id]\n",
                "    image_info = image_id_to_info.get(image_id)\n",
                "    if not image_info:\n",
                "        continue\n",
                "    \n",
                "    image_path = os.path.join(IMAGE_BASE_PATH, image_info['file_name'])\n",
                "    try:\n",
                "        img = Image.open(image_path).convert('RGB')\n",
                "        x_min, y_min, width, height = [int(b) for b in bbox]\n",
                "        x_max, y_max = x_min + width, y_min + height\n",
                "        x_min, y_min = max(0, x_min), max(0, y_min)\n",
                "        x_max, y_max = min(img.width, x_max), min(img.height, y_max)\n",
                "        \n",
                "        cropped = img.crop((x_min, y_min, x_max, y_max))\n",
                "        resized = cropped.resize((IMG_SIZE, IMG_SIZE), Image.LANCZOS)\n",
                "        \n",
                "        output_filename = f\"{image_id}_{ann['id']}.png\"\n",
                "        output_path = os.path.join(OUTPUT_DIR, category_name, output_filename)\n",
                "        resized.save(output_path)\n",
                "        class_counts[category_name] += 1\n",
                "    except Exception as e:\n",
                "        print(f\"Error processing {image_path}: {e}\")\n",
                "\n",
                "print(\"\\nâœ… Cropping complete!\")\n",
                "for cat in sorted(class_counts.keys()):\n",
                "    print(f\"  Class {cat}: {class_counts[cat]} images\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "gan_section"
            },
            "source": [
                "## 3. GAN Architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "hyperparams"
            },
            "outputs": [],
            "source": [
                "LATENT_DIM = 100\n",
                "NUM_CLASSES = 6\n",
                "EMBED_DIM = 50\n",
                "NGF = 64\n",
                "NDF = 64\n",
                "NC = 3\n",
                "BATCH_SIZE = 32\n",
                "NUM_EPOCHS = 200\n",
                "LR = 0.0002\n",
                "BETA1 = 0.5"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "generator"
            },
            "outputs": [],
            "source": [
                "class Generator(nn.Module):\n",
                "    def __init__(self, latent_dim, num_classes, embed_dim, ngf, nc):\n",
                "        super(Generator, self).__init__()\n",
                "        self.label_embedding = nn.Embedding(num_classes, embed_dim)\n",
                "        input_dim = latent_dim + embed_dim\n",
                "        \n",
                "        self.main = nn.Sequential(\n",
                "            nn.ConvTranspose2d(input_dim, ngf * 8, 4, 1, 0, bias=False),\n",
                "            nn.BatchNorm2d(ngf * 8),\n",
                "            nn.ReLU(True),\n",
                "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
                "            nn.BatchNorm2d(ngf * 4),\n",
                "            nn.ReLU(True),\n",
                "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
                "            nn.BatchNorm2d(ngf * 2),\n",
                "            nn.ReLU(True),\n",
                "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
                "            nn.BatchNorm2d(ngf),\n",
                "            nn.ReLU(True),\n",
                "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
                "            nn.Tanh()\n",
                "        )\n",
                "    \n",
                "    def forward(self, noise, labels):\n",
                "        label_embed = self.label_embedding(labels)\n",
                "        x = torch.cat([noise, label_embed], dim=1)\n",
                "        x = x.view(x.size(0), -1, 1, 1)\n",
                "        return self.main(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "discriminator"
            },
            "outputs": [],
            "source": [
                "class Discriminator(nn.Module):\n",
                "    def __init__(self, num_classes, ndf, nc, img_size):\n",
                "        super(Discriminator, self).__init__()\n",
                "        self.label_embedding = nn.Embedding(num_classes, img_size * img_size)\n",
                "        self.img_size = img_size\n",
                "        \n",
                "        self.main = nn.Sequential(\n",
                "            nn.Conv2d(nc + 1, ndf, 4, 2, 1, bias=False),\n",
                "            nn.LeakyReLU(0.2, inplace=True),\n",
                "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
                "            nn.BatchNorm2d(ndf * 2),\n",
                "            nn.LeakyReLU(0.2, inplace=True),\n",
                "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
                "            nn.BatchNorm2d(ndf * 4),\n",
                "            nn.LeakyReLU(0.2, inplace=True),\n",
                "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
                "            nn.BatchNorm2d(ndf * 8),\n",
                "            nn.LeakyReLU(0.2, inplace=True),\n",
                "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
                "            nn.Sigmoid()\n",
                "        )\n",
                "    \n",
                "    def forward(self, img, labels):\n",
                "        label_embed = self.label_embedding(labels)\n",
                "        label_channel = label_embed.view(labels.size(0), 1, self.img_size, self.img_size)\n",
                "        x = torch.cat([img, label_channel], dim=1)\n",
                "        return self.main(x).view(-1, 1).squeeze(1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "dataset_section"
            },
            "source": [
                "## 4. Dataset & DataLoader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "dataset"
            },
            "outputs": [],
            "source": [
                "class DiceDataset(Dataset):\n",
                "    def __init__(self, root_dir, transform=None):\n",
                "        self.root_dir = root_dir\n",
                "        self.transform = transform\n",
                "        self.samples = []\n",
                "        \n",
                "        for label_idx, label_name in enumerate(['1', '2', '3', '4', '5', '6']):\n",
                "            label_dir = os.path.join(root_dir, label_name)\n",
                "            if os.path.exists(label_dir):\n",
                "                for img_name in os.listdir(label_dir):\n",
                "                    if img_name.endswith(('.png', '.jpg', '.jpeg')):\n",
                "                        self.samples.append((os.path.join(label_dir, img_name), label_idx))\n",
                "        print(f\"Loaded {len(self.samples)} samples\")\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_path, label = self.samples[idx]\n",
                "        image = Image.open(img_path).convert('RGB')\n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "        return image, label\n",
                "\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize(IMG_SIZE),\n",
                "    transforms.CenterCrop(IMG_SIZE),\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.RandomRotation(15),\n",
                "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
                "])\n",
                "\n",
                "dice_dataset = DiceDataset(OUTPUT_DIR, transform=transform)\n",
                "dataloader = DataLoader(dice_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "training_section"
            },
            "source": [
                "## 5. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "init_models"
            },
            "outputs": [],
            "source": [
                "def weights_init(m):\n",
                "    classname = m.__class__.__name__\n",
                "    if classname.find('Conv') != -1:\n",
                "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
                "    elif classname.find('BatchNorm') != -1:\n",
                "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
                "        nn.init.constant_(m.bias.data, 0)\n",
                "\n",
                "netG = Generator(LATENT_DIM, NUM_CLASSES, EMBED_DIM, NGF, NC).to(device)\n",
                "netD = Discriminator(NUM_CLASSES, NDF, NC, IMG_SIZE).to(device)\n",
                "netG.apply(weights_init)\n",
                "netD.apply(weights_init)\n",
                "\n",
                "print(f\"Generator params: {sum(p.numel() for p in netG.parameters()):,}\")\n",
                "print(f\"Discriminator params: {sum(p.numel() for p in netD.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "training_loop"
            },
            "outputs": [],
            "source": [
                "criterion = nn.BCELoss()\n",
                "optimizerD = optim.Adam(netD.parameters(), lr=LR, betas=(BETA1, 0.999))\n",
                "optimizerG = optim.Adam(netG.parameters(), lr=LR, betas=(BETA1, 0.999))\n",
                "\n",
                "G_losses, D_losses = [], []\n",
                "\n",
                "print(\"Starting Training...\")\n",
                "for epoch in range(NUM_EPOCHS):\n",
                "    epoch_D_loss, epoch_G_loss = 0, 0\n",
                "    \n",
                "    for real_imgs, labels in dataloader:\n",
                "        real_imgs, labels = real_imgs.to(device), labels.to(device)\n",
                "        batch_size = real_imgs.size(0)\n",
                "        \n",
                "        real_label = torch.ones(batch_size, device=device) * 0.9\n",
                "        fake_label = torch.zeros(batch_size, device=device) + 0.1\n",
                "        \n",
                "        # Train Discriminator\n",
                "        netD.zero_grad()\n",
                "        output_real = netD(real_imgs, labels)\n",
                "        errD_real = criterion(output_real, real_label)\n",
                "        errD_real.backward()\n",
                "        \n",
                "        noise = torch.randn(batch_size, LATENT_DIM, device=device)\n",
                "        fake_imgs = netG(noise, labels)\n",
                "        output_fake = netD(fake_imgs.detach(), labels)\n",
                "        errD_fake = criterion(output_fake, fake_label)\n",
                "        errD_fake.backward()\n",
                "        optimizerD.step()\n",
                "        \n",
                "        # Train Generator\n",
                "        netG.zero_grad()\n",
                "        output = netD(fake_imgs, labels)\n",
                "        errG = criterion(output, real_label)\n",
                "        errG.backward()\n",
                "        optimizerG.step()\n",
                "        \n",
                "        epoch_D_loss += (errD_real + errD_fake).item()\n",
                "        epoch_G_loss += errG.item()\n",
                "    \n",
                "    G_losses.append(epoch_G_loss / len(dataloader))\n",
                "    D_losses.append(epoch_D_loss / len(dataloader))\n",
                "    \n",
                "    if (epoch + 1) % 20 == 0 or epoch == 0:\n",
                "        print(f\"[{epoch+1:3d}/{NUM_EPOCHS}] Loss_D: {D_losses[-1]:.4f} | Loss_G: {G_losses[-1]:.4f}\")\n",
                "\n",
                "print(\"\\nâœ… Training complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "plot_losses"
            },
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(G_losses, label=\"Generator\")\n",
                "plt.plot(D_losses, label=\"Discriminator\")\n",
                "plt.xlabel(\"Epoch\")\n",
                "plt.ylabel(\"Loss\")\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.savefig('training_losses.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "coco_section"
            },
            "source": [
                "## 6. Generate COCO-Format Dataset\n",
                "\n",
                "Create full scene images with dice on backgrounds and COCO annotations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "extract_backgrounds"
            },
            "outputs": [],
            "source": [
                "# Extract background images from original dataset\n",
                "BACKGROUND_DIR = 'backgrounds'\n",
                "os.makedirs(BACKGROUND_DIR, exist_ok=True)\n",
                "\n",
                "print(\"Extracting background samples from training images...\")\n",
                "train_images_dir = f'{dataset.location}/train'\n",
                "bg_count = 0\n",
                "\n",
                "for img_file in os.listdir(train_images_dir):\n",
                "    if img_file.endswith(('.jpg', '.png', '.jpeg')):\n",
                "        img = Image.open(os.path.join(train_images_dir, img_file)).convert('RGB')\n",
                "        img.save(os.path.join(BACKGROUND_DIR, f'bg_{bg_count:04d}.jpg'))\n",
                "        bg_count += 1\n",
                "        if bg_count >= 50:\n",
                "            break\n",
                "\n",
                "print(f\"Extracted {bg_count} background images\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "generate_coco"
            },
            "outputs": [],
            "source": [
                "# Configuration\n",
                "SYNTHETIC_COCO_DIR = 'synthetic_coco_dataset'\n",
                "SCENE_SIZE = (640, 640)\n",
                "DICE_SIZE_RANGE = (60, 120)\n",
                "DICE_PER_IMAGE = (1, 4)\n",
                "\n",
                "os.makedirs(os.path.join(SYNTHETIC_COCO_DIR, 'train'), exist_ok=True)\n",
                "\n",
                "# Calculate images needed per class\n",
                "current_counts = {str(i): class_counts.get(str(i), 0) for i in range(1, 7)}\n",
                "target_count = max(current_counts.values())\n",
                "images_to_generate = {k: max(0, target_count - v) for k, v in current_counts.items()}\n",
                "\n",
                "print(f\"Target count per class: {target_count}\")\n",
                "total_synthetic_images = sum(images_to_generate.values()) // 2 + 50  # Estimate\n",
                "print(f\"Will generate approximately {total_synthetic_images} full scene images\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "create_scenes"
            },
            "outputs": [],
            "source": [
                "def generate_dice_image(generator, class_idx, size):\n",
                "    \"\"\"Generate a single dice image of given class and resize\"\"\"\n",
                "    generator.eval()\n",
                "    with torch.no_grad():\n",
                "        noise = torch.randn(1, LATENT_DIM, device=device)\n",
                "        label = torch.tensor([class_idx], device=device)\n",
                "        fake_img = generator(noise, label)[0].cpu().numpy().transpose(1, 2, 0)\n",
                "        fake_img = ((fake_img + 1) / 2 * 255).astype(np.uint8)\n",
                "        fake_img = np.clip(fake_img, 0, 255)\n",
                "        pil_img = Image.fromarray(fake_img)\n",
                "        return pil_img.resize((size, size), Image.LANCZOS)\n",
                "\n",
                "def check_overlap(new_box, existing_boxes, min_distance=10):\n",
                "    \"\"\"Check if new box overlaps with existing boxes\"\"\"\n",
                "    for box in existing_boxes:\n",
                "        if (new_box[0] < box[2] + min_distance and new_box[2] > box[0] - min_distance and\n",
                "            new_box[1] < box[3] + min_distance and new_box[3] > box[1] - min_distance):\n",
                "            return True\n",
                "    return False\n",
                "\n",
                "# Load backgrounds\n",
                "background_files = [f for f in os.listdir(BACKGROUND_DIR) if f.endswith(('.jpg', '.png'))]\n",
                "\n",
                "# COCO format structures\n",
                "coco_images = []\n",
                "coco_annotations = []\n",
                "coco_categories = [{'id': i, 'name': str(i)} for i in range(1, 7)]\n",
                "\n",
                "image_id = 1\n",
                "annotation_id = 1\n",
                "generated_per_class = {str(i): 0 for i in range(1, 7)}\n",
                "\n",
                "print(\"\\nðŸŽ¨ Generating synthetic COCO dataset...\")\n",
                "\n",
                "for scene_idx in tqdm(range(total_synthetic_images)):\n",
                "    # Load random background and resize\n",
                "    bg_file = random.choice(background_files)\n",
                "    background = Image.open(os.path.join(BACKGROUND_DIR, bg_file)).convert('RGB')\n",
                "    background = background.resize(SCENE_SIZE, Image.LANCZOS)\n",
                "    scene = background.copy()\n",
                "    \n",
                "    # Determine number of dice and which classes to generate\n",
                "    num_dice = random.randint(*DICE_PER_IMAGE)\n",
                "    \n",
                "    # Prioritize underrepresented classes\n",
                "    needed_classes = [k for k, v in images_to_generate.items() if generated_per_class[k] < v]\n",
                "    if not needed_classes:\n",
                "        needed_classes = [str(i) for i in range(1, 7)]\n",
                "    \n",
                "    placed_boxes = []\n",
                "    scene_annotations = []\n",
                "    \n",
                "    for _ in range(num_dice):\n",
                "        # Select class\n",
                "        class_name = random.choice(needed_classes)\n",
                "        class_idx = int(class_name) - 1\n",
                "        \n",
                "        # Random dice size and position\n",
                "        dice_size = random.randint(*DICE_SIZE_RANGE)\n",
                "        \n",
                "        # Try to place dice without overlap\n",
                "        max_attempts = 20\n",
                "        for attempt in range(max_attempts):\n",
                "            x = random.randint(0, SCENE_SIZE[0] - dice_size)\n",
                "            y = random.randint(0, SCENE_SIZE[1] - dice_size)\n",
                "            new_box = [x, y, x + dice_size, y + dice_size]\n",
                "            \n",
                "            if not check_overlap(new_box, placed_boxes):\n",
                "                # Generate and paste dice\n",
                "                dice_img = generate_dice_image(netG, class_idx, dice_size)\n",
                "                scene.paste(dice_img, (x, y))\n",
                "                \n",
                "                placed_boxes.append(new_box)\n",
                "                scene_annotations.append({\n",
                "                    'id': annotation_id,\n",
                "                    'image_id': image_id,\n",
                "                    'category_id': int(class_name),\n",
                "                    'bbox': [x, y, dice_size, dice_size],  # COCO format: x, y, w, h\n",
                "                    'area': dice_size * dice_size,\n",
                "                    'iscrowd': 0\n",
                "                })\n",
                "                annotation_id += 1\n",
                "                generated_per_class[class_name] += 1\n",
                "                break\n",
                "    \n",
                "    if scene_annotations:\n",
                "        # Save image\n",
                "        img_filename = f\"synthetic_{image_id:05d}.jpg\"\n",
                "        scene.save(os.path.join(SYNTHETIC_COCO_DIR, 'train', img_filename))\n",
                "        \n",
                "        coco_images.append({\n",
                "            'id': image_id,\n",
                "            'file_name': img_filename,\n",
                "            'width': SCENE_SIZE[0],\n",
                "            'height': SCENE_SIZE[1]\n",
                "        })\n",
                "        coco_annotations.extend(scene_annotations)\n",
                "        image_id += 1\n",
                "\n",
                "# Save COCO annotations\n",
                "coco_data = {\n",
                "    'images': coco_images,\n",
                "    'annotations': coco_annotations,\n",
                "    'categories': coco_categories\n",
                "}\n",
                "\n",
                "with open(os.path.join(SYNTHETIC_COCO_DIR, 'train', '_annotations.coco.json'), 'w') as f:\n",
                "    json.dump(coco_data, f, indent=2)\n",
                "\n",
                "print(f\"\\nâœ… Generated {len(coco_images)} images with {len(coco_annotations)} annotations\")\n",
                "print(f\"\\nPer-class generation counts:\")\n",
                "for k, v in generated_per_class.items():\n",
                "    print(f\"  Class {k}: {v} dice\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "visualize"
            },
            "outputs": [],
            "source": [
                "# Visualize sample generated scenes\n",
                "import matplotlib.patches as patches\n",
                "\n",
                "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "sample_images = random.sample(coco_images, min(6, len(coco_images)))\n",
                "\n",
                "for ax, img_info in zip(axes, sample_images):\n",
                "    img_path = os.path.join(SYNTHETIC_COCO_DIR, 'train', img_info['file_name'])\n",
                "    img = Image.open(img_path)\n",
                "    ax.imshow(img)\n",
                "    \n",
                "    # Draw bounding boxes\n",
                "    for ann in coco_annotations:\n",
                "        if ann['image_id'] == img_info['id']:\n",
                "            x, y, w, h = ann['bbox']\n",
                "            rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='lime', facecolor='none')\n",
                "            ax.add_patch(rect)\n",
                "            ax.text(x, y - 5, f\"Dice {ann['category_id']}\", color='lime', fontsize=10, weight='bold')\n",
                "    \n",
                "    ax.axis('off')\n",
                "    ax.set_title(img_info['file_name'])\n",
                "\n",
                "plt.suptitle('Sample Generated Scenes with COCO Annotations', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig('sample_coco_scenes.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "verify_section"
            },
            "source": [
                "## 7. Verify Compatibility with DiceDetectionDataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "verify"
            },
            "outputs": [],
            "source": [
                "# Test loading with DiceDetectionDataset\n",
                "import sys\n",
                "sys.path.append('./src')\n",
                "\n",
                "try:\n",
                "    from src.dataset import DiceDetectionDataset\n",
                "    \n",
                "    synthetic_dataset = DiceDetectionDataset(\n",
                "        root_dir=os.path.join(SYNTHETIC_COCO_DIR, 'train'),\n",
                "        annotation_file='_annotations.coco.json',\n",
                "        split='train'\n",
                "    )\n",
                "    \n",
                "    print(f\"âœ… Successfully loaded synthetic dataset!\")\n",
                "    print(f\"   Number of images: {len(synthetic_dataset)}\")\n",
                "    print(f\"   Number of classes: {synthetic_dataset.num_classes}\")\n",
                "    print(f\"   Class distribution: {synthetic_dataset.get_class_distribution()}\")\n",
                "    \n",
                "    # Test getting an item\n",
                "    image, target = synthetic_dataset[0]\n",
                "    print(f\"\\n   Sample image shape: {image.shape}\")\n",
                "    print(f\"   Sample boxes: {target['boxes'].shape}\")\n",
                "    print(f\"   Sample labels: {target['labels']}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Could not test with DiceDetectionDataset: {e}\")\n",
                "    print(\"The dataset structure is still COCO-compatible.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "save_model"
            },
            "source": [
                "## 8. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "save"
            },
            "outputs": [],
            "source": [
                "MODEL_DIR = 'gan_models'\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "\n",
                "torch.save({\n",
                "    'generator_state_dict': netG.state_dict(),\n",
                "    'discriminator_state_dict': netD.state_dict(),\n",
                "    'epoch': NUM_EPOCHS,\n",
                "    'G_losses': G_losses,\n",
                "    'D_losses': D_losses,\n",
                "}, os.path.join(MODEL_DIR, 'conditional_dcgan_dice.pth'))\n",
                "\n",
                "print(f\"âœ… Model saved to {MODEL_DIR}/conditional_dcgan_dice.pth\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "summary"
            },
            "source": [
                "## ðŸš€ Usage\n",
                "\n",
                "The synthetic dataset is now ready for use with `3_augmentation_comparison.ipynb`:\n",
                "\n",
                "```python\n",
                "from src.dataset import DiceDetectionDataset\n",
                "\n",
                "synthetic_train = DiceDetectionDataset(\n",
                "    root_dir='synthetic_coco_dataset/train',\n",
                "    annotation_file='_annotations.coco.json',\n",
                "    split='train'\n",
                ")\n",
                "```\n",
                "\n",
                "**Generated files:**\n",
                "- `synthetic_coco_dataset/train/` - Images with dice on backgrounds\n",
                "- `synthetic_coco_dataset/train/_annotations.coco.json` - COCO annotations\n",
                "- `gan_models/conditional_dcgan_dice.pth` - Trained GAN"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}