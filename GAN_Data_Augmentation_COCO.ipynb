{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro"
            },
            "source": [
                "# ðŸŽ² GAN Data Augmentation (COCO Format)\n",
                "\n",
                "This notebook trains a **Conditional DCGAN** and generates synthetic full-scene images with COCO annotations.\n",
                "The output is directly compatible with `DiceDetectionDataset` used in `3_augmentation_comparison.ipynb`.\n",
                "\n",
                "**Key Features:**\n",
                "- Uses local balanced annotations (`Annotations/train_image_balanced.coco.json`) - Zipfian balanced on images\n",
                "- Downloads images from Roboflow\n",
                "- Trains conditional DCGAN on dice crops\n",
                "- Generates full scene images with dice placed on backgrounds\n",
                "- Outputs COCO-format `_annotations.coco.json` file"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup"
            },
            "source": [
                "## 1. Setup & Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install"
            },
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
                "!pip install roboflow pillow matplotlib seaborn tqdm numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "clone"
            },
            "outputs": [],
            "source": [
                "# Clone the git repository with the code\n",
                "import os\n",
                "if not os.path.exists('Dice-Detection'):\n",
                "    !git clone https://github.com/Adr44mo/Dice-Detection.git\n",
                "    %cd Dice-Detection\n",
                "else:\n",
                "    %cd Dice-Detection\n",
                "\n",
                "# Add src to path\n",
                "import sys\n",
                "sys.path.append('./src')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "imports"
            },
            "source": [
                "## 2. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "import_libs"
            },
            "outputs": [],
            "source": [
                "import json\n",
                "import random\n",
                "import numpy as np\n",
                "from collections import Counter\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "from tqdm import tqdm\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import torchvision.transforms as transforms\n",
                "import torchvision.utils as vutils\n",
                "\n",
                "# Import custom modules from src\n",
                "from src.gan import (\n",
                "    Generator,\n",
                "    Discriminator,\n",
                "    weights_init,\n",
                "    generate_dice_image,\n",
                "    extract_backgrounds,\n",
                "    create_synthetic_coco_dataset\n",
                ")\n",
                "from src.dataset import DiceDetectionDataset  # For verification\n",
                "\n",
                "# Set random seeds for reproducibility\n",
                "SEED = 42\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "download"
            },
            "source": [
                "## 3. Download Dataset from Roboflow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "roboflow"
            },
            "outputs": [],
            "source": [
                "from roboflow import Roboflow\n",
                "\n",
                "rf = Roboflow(api_key=\"kd9lS9tvh5StEQtSA6i9\")\n",
                "project = rf.workspace(\"workspace-spezm\").project(\"dice-0sexk\")\n",
                "dataset = project.version(2).download(\"coco\")\n",
                "print(f\"Dataset downloaded to: {dataset.location}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "load_annotations"
            },
            "source": [
                "## 4. Load Balanced Annotations\n",
                "\n",
                "We use the local balanced annotations (`train_image_balanced.coco.json`) which contains a Zipfian-balanced subset of images, while using the images downloaded from Roboflow."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "balanced_ann"
            },
            "outputs": [],
            "source": [
                "# Load balanced annotations from local file\n",
                "BALANCED_ANNOTATION_FILE = 'Annotations/train_image_balanced.coco.json'\n",
                "IMAGE_BASE_PATH = f'{dataset.location}/train'\n",
                "\n",
                "with open(BALANCED_ANNOTATION_FILE, 'r') as f:\n",
                "    balanced_annotations = json.load(f)\n",
                "\n",
                "print(f\"Loaded balanced annotations from: {BALANCED_ANNOTATION_FILE}\")\n",
                "print(f\"Images: {len(balanced_annotations['images'])}\")\n",
                "print(f\"Annotations: {len(balanced_annotations['annotations'])}\")\n",
                "print(f\"Categories: {len(balanced_annotations['categories'])}\")\n",
                "\n",
                "# Extract categories (only numeric dice classes)\n",
                "categories = {cat['id']: cat['name'] for cat in balanced_annotations['categories']}\n",
                "valid_categories = {k: v for k, v in categories.items() if v.isdigit()}\n",
                "print(f\"\\nValid dice categories: {valid_categories}\")\n",
                "\n",
                "# Create image lookup\n",
                "image_id_to_info = {\n",
                "    img['id']: {'file_name': img['file_name'], 'width': img['width'], 'height': img['height']}\n",
                "    for img in balanced_annotations['images']\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "crop"
            },
            "source": [
                "## 5. Extract Dice Crops\n",
                "\n",
                "Crop dice from downloaded images using bounding boxes from balanced annotations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "crop_dice"
            },
            "outputs": [],
            "source": [
                "IMG_SIZE = 64\n",
                "OUTPUT_DIR = 'gan_training_data'\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "for cat_name in valid_categories.values():\n",
                "    os.makedirs(os.path.join(OUTPUT_DIR, cat_name), exist_ok=True)\n",
                "\n",
                "class_counts = Counter()\n",
                "print(\"\\nCropping dice images from balanced dataset...\")\n",
                "\n",
                "for ann in tqdm(balanced_annotations['annotations']):\n",
                "    category_id = ann['category_id']\n",
                "    if category_id not in valid_categories:\n",
                "        continue\n",
                "    \n",
                "    image_id = ann['image_id']\n",
                "    bbox = ann['bbox']\n",
                "    category_name = valid_categories[category_id]\n",
                "    image_info = image_id_to_info.get(image_id)\n",
                "    \n",
                "    if not image_info:\n",
                "        continue\n",
                "    \n",
                "    image_path = os.path.join(IMAGE_BASE_PATH, image_info['file_name'])\n",
                "    \n",
                "    try:\n",
                "        img = Image.open(image_path).convert('RGB')\n",
                "        x_min, y_min, width, height = [int(b) for b in bbox]\n",
                "        x_max, y_max = x_min + width, y_min + height\n",
                "        x_min, y_min = max(0, x_min), max(0, y_min)\n",
                "        x_max, y_max = min(img.width, x_max), min(img.height, y_max)\n",
                "        \n",
                "        cropped = img.crop((x_min, y_min, x_max, y_max))\n",
                "        resized = cropped.resize((IMG_SIZE, IMG_SIZE), Image.LANCZOS)\n",
                "        \n",
                "        output_filename = f\"{image_id}_{ann['id']}.png\"\n",
                "        output_path = os.path.join(OUTPUT_DIR, category_name, output_filename)\n",
                "        resized.save(output_path)\n",
                "        class_counts[category_name] += 1\n",
                "    except Exception as e:\n",
                "        print(f\"Error processing {image_path}: {e}\")\n",
                "\n",
                "print(\"\\nâœ… Cropping complete!\")\n",
                "for cat in sorted(class_counts.keys()):\n",
                "    print(f\"  Class {cat}: {class_counts[cat]} images\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "gan_section"
            },
            "source": [
                "## 6. GAN Architecture & Hyperparameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "hyperparams"
            },
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "LATENT_DIM = 100\n",
                "NUM_CLASSES = 6\n",
                "EMBED_DIM = 50\n",
                "NGF = 64\n",
                "NDF = 64\n",
                "NC = 3\n",
                "BATCH_SIZE = 32\n",
                "NUM_EPOCHS = 200\n",
                "LR = 0.0002\n",
                "BETA1 = 0.5\n",
                "\n",
                "print(f\"Latent dimension: {LATENT_DIM}\")\n",
                "print(f\"Number of classes: {NUM_CLASSES}\")\n",
                "print(f\"Batch size: {BATCH_SIZE}\")\n",
                "print(f\"Number of epochs: {NUM_EPOCHS}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "init_models"
            },
            "outputs": [],
            "source": [
                "# Initialize models from src.gan\n",
                "netG = Generator(LATENT_DIM, NUM_CLASSES, EMBED_DIM, NGF, NC).to(device)\n",
                "netD = Discriminator(NUM_CLASSES, NDF, NC, IMG_SIZE).to(device)\n",
                "\n",
                "# Apply weight initialization\n",
                "netG.apply(weights_init)\n",
                "netD.apply(weights_init)\n",
                "\n",
                "print(f\"Generator params: {sum(p.numel() for p in netG.parameters()):,}\")\n",
                "print(f\"Discriminator params: {sum(p.numel() for p in netD.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "dataset_section"
            },
            "source": [
                "## 7. Prepare Dataset & DataLoader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "dataset"
            },
            "outputs": [],
            "source": [
                "class DiceDataset(Dataset):\n",
                "    \"\"\"Dataset for loading cropped dice images for GAN training.\"\"\"\n",
                "    \n",
                "    def __init__(self, root_dir, transform=None):\n",
                "        self.root_dir = root_dir\n",
                "        self.transform = transform\n",
                "        self.samples = []\n",
                "        \n",
                "        for label_idx, label_name in enumerate(['1', '2', '3', '4', '5', '6']):\n",
                "            label_dir = os.path.join(root_dir, label_name)\n",
                "            if os.path.exists(label_dir):\n",
                "                for img_name in os.listdir(label_dir):\n",
                "                    if img_name.endswith(('.png', '.jpg', '.jpeg')):\n",
                "                        self.samples.append((os.path.join(label_dir, img_name), label_idx))\n",
                "        print(f\"Loaded {len(self.samples)} samples\")\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_path, label = self.samples[idx]\n",
                "        image = Image.open(img_path).convert('RGB')\n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "        return image, label\n",
                "\n",
                "# Data transforms\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize(IMG_SIZE),\n",
                "    transforms.CenterCrop(IMG_SIZE),\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.RandomRotation(15),\n",
                "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
                "])\n",
                "\n",
                "dice_dataset = DiceDataset(OUTPUT_DIR, transform=transform)\n",
                "dataloader = DataLoader(dice_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
                "\n",
                "print(f\"Number of batches: {len(dataloader)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "training_section"
            },
            "source": [
                "## 8. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "training_loop"
            },
            "outputs": [],
            "source": [
                "criterion = nn.BCELoss()\n",
                "optimizerD = optim.Adam(netD.parameters(), lr=LR, betas=(BETA1, 0.999))\n",
                "optimizerG = optim.Adam(netG.parameters(), lr=LR, betas=(BETA1, 0.999))\n",
                "\n",
                "G_losses, D_losses = [], []\n",
                "\n",
                "print(\"Starting Training...\")\n",
                "for epoch in range(NUM_EPOCHS):\n",
                "    epoch_D_loss, epoch_G_loss = 0, 0\n",
                "    \n",
                "    for real_imgs, labels in dataloader:\n",
                "        real_imgs, labels = real_imgs.to(device), labels.to(device)\n",
                "        batch_size = real_imgs.size(0)\n",
                "        \n",
                "        real_label = torch.ones(batch_size, device=device) * 0.9\n",
                "        fake_label = torch.zeros(batch_size, device=device) + 0.1\n",
                "        \n",
                "        # Train Discriminator\n",
                "        netD.zero_grad()\n",
                "        output_real = netD(real_imgs, labels)\n",
                "        errD_real = criterion(output_real, real_label)\n",
                "        errD_real.backward()\n",
                "        \n",
                "        noise = torch.randn(batch_size, LATENT_DIM, device=device)\n",
                "        fake_imgs = netG(noise, labels)\n",
                "        output_fake = netD(fake_imgs.detach(), labels)\n",
                "        errD_fake = criterion(output_fake, fake_label)\n",
                "        errD_fake.backward()\n",
                "        optimizerD.step()\n",
                "        \n",
                "        # Train Generator\n",
                "        netG.zero_grad()\n",
                "        output = netD(fake_imgs, labels)\n",
                "        errG = criterion(output, real_label)\n",
                "        errG.backward()\n",
                "        optimizerG.step()\n",
                "        \n",
                "        epoch_D_loss += (errD_real + errD_fake).item()\n",
                "        epoch_G_loss += errG.item()\n",
                "    \n",
                "    G_losses.append(epoch_G_loss / len(dataloader))\n",
                "    D_losses.append(epoch_D_loss / len(dataloader))\n",
                "    \n",
                "    if (epoch + 1) % 20 == 0 or epoch == 0:\n",
                "        print(f\"[{epoch+1:3d}/{NUM_EPOCHS}] Loss_D: {D_losses[-1]:.4f} | Loss_G: {G_losses[-1]:.4f}\")\n",
                "\n",
                "print(\"\\nâœ… Training complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "plot_losses"
            },
            "outputs": [],
            "source": [
                "# Plot training losses\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(G_losses, label=\"Generator\")\n",
                "plt.plot(D_losses, label=\"Discriminator\")\n",
                "plt.xlabel(\"Epoch\")\n",
                "plt.ylabel(\"Loss\")\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.savefig('training_losses.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "coco_section"
            },
            "source": [
                "## 9. Generate COCO-Format Dataset\n",
                "\n",
                "Create full scene images with dice on backgrounds and COCO annotations using functions from `src.gan`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "extract_backgrounds"
            },
            "outputs": [],
            "source": [
                "# Extract background images from downloaded dataset\n",
                "BACKGROUND_DIR = 'backgrounds'\n",
                "train_images_dir = f'{dataset.location}/train'\n",
                "\n",
                "print(\"Extracting background samples from training images...\")\n",
                "bg_count = extract_backgrounds(train_images_dir, BACKGROUND_DIR, num_backgrounds=50)\n",
                "print(f\"Extracted {bg_count} background images\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "generate_coco"
            },
            "outputs": [],
            "source": [
                "# Configuration for synthetic dataset generation\n",
                "SYNTHETIC_COCO_DIR = 'synthetic_coco_dataset'\n",
                "\n",
                "# Calculate images needed per class for balancing\n",
                "current_counts = {str(i): class_counts.get(str(i), 0) for i in range(1, 7)}\n",
                "target_count = max(current_counts.values())\n",
                "images_to_generate = {k: max(0, target_count - v) for k, v in current_counts.items()}\n",
                "\n",
                "print(f\"Current class distribution: {current_counts}\")\n",
                "print(f\"Target count per class: {target_count}\")\n",
                "total_synthetic_images = sum(images_to_generate.values()) // 2 + 50\n",
                "print(f\"Will generate approximately {total_synthetic_images} full scene images\")\n",
                "\n",
                "# Generation config\n",
                "gen_config = {\n",
                "    'scene_size': (640, 640),\n",
                "    'dice_size_range': (60, 120),\n",
                "    'dice_per_image': (1, 4),\n",
                "    'num_images': total_synthetic_images\n",
                "}\n",
                "\n",
                "# Generate synthetic COCO dataset\n",
                "coco_data = create_synthetic_coco_dataset(\n",
                "    generator=netG,\n",
                "    background_dir=BACKGROUND_DIR,\n",
                "    output_dir=SYNTHETIC_COCO_DIR,\n",
                "    config=gen_config,\n",
                "    device=device,\n",
                "    class_counts=current_counts,\n",
                "    latent_dim=LATENT_DIM\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "visualize"
            },
            "outputs": [],
            "source": [
                "# Visualize sample generated scenes\n",
                "import matplotlib.patches as patches\n",
                "\n",
                "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "sample_images = random.sample(coco_data['images'], min(6, len(coco_data['images'])))\n",
                "\n",
                "for ax, img_info in zip(axes, sample_images):\n",
                "    img_path = os.path.join(SYNTHETIC_COCO_DIR, 'train', img_info['file_name'])\n",
                "    img = Image.open(img_path)\n",
                "    ax.imshow(img)\n",
                "    \n",
                "    # Draw bounding boxes\n",
                "    for ann in coco_data['annotations']:\n",
                "        if ann['image_id'] == img_info['id']:\n",
                "            x, y, w, h = ann['bbox']\n",
                "            rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='lime', facecolor='none')\n",
                "            ax.add_patch(rect)\n",
                "            ax.text(x, y - 5, f\"Dice {ann['category_id']}\", color='lime', fontsize=10, weight='bold')\n",
                "    \n",
                "    ax.axis('off')\n",
                "    ax.set_title(img_info['file_name'])\n",
                "\n",
                "plt.suptitle('Sample Generated Scenes with COCO Annotations', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig('sample_coco_scenes.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "verify_section"
            },
            "source": [
                "## 10. Verify Compatibility with DiceDetectionDataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "verify"
            },
            "outputs": [],
            "source": [
                "# Test loading with DiceDetectionDataset from src\n",
                "try:\n",
                "    synthetic_dataset = DiceDetectionDataset(\n",
                "        root_dir=os.path.join(SYNTHETIC_COCO_DIR, 'train'),\n",
                "        annotation_file='_annotations.coco.json',\n",
                "        split='train'\n",
                "    )\n",
                "    \n",
                "    print(f\"âœ… Successfully loaded synthetic dataset!\")\n",
                "    print(f\"   Number of images: {len(synthetic_dataset)}\")\n",
                "    print(f\"   Number of classes: {synthetic_dataset.num_classes}\")\n",
                "    print(f\"   Class distribution: {synthetic_dataset.get_class_distribution()}\")\n",
                "    \n",
                "    # Test getting an item\n",
                "    image, target = synthetic_dataset[0]\n",
                "    print(f\"\\n   Sample image shape: {image.shape}\")\n",
                "    print(f\"   Sample boxes: {target['boxes'].shape}\")\n",
                "    print(f\"   Sample labels: {target['labels']}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Could not test with DiceDetectionDataset: {e}\")\n",
                "    print(\"The dataset structure is still COCO-compatible.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "save_model"
            },
            "source": [
                "## 11. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "save"
            },
            "outputs": [],
            "source": [
                "MODEL_DIR = 'gan_models'\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "\n",
                "torch.save({\n",
                "    'generator_state_dict': netG.state_dict(),\n",
                "    'discriminator_state_dict': netD.state_dict(),\n",
                "    'epoch': NUM_EPOCHS,\n",
                "    'G_losses': G_losses,\n",
                "    'D_losses': D_losses,\n",
                "}, os.path.join(MODEL_DIR, 'conditional_dcgan_dice.pth'))\n",
                "\n",
                "print(f\"âœ… Model saved to {MODEL_DIR}/conditional_dcgan_dice.pth\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "summary"
            },
            "source": [
                "## ðŸš€ Summary\n",
                "\n",
                "This notebook:\n",
                "1. âœ… Downloaded images from Roboflow\n",
                "2. âœ… Used local balanced annotations (`Annotations/train_image_balanced.coco.json`) - Zipfian balanced on images\n",
                "3. âœ… Extracted dice crops from balanced subset\n",
                "4. âœ… Trained Conditional DCGAN on dice crops\n",
                "5. âœ… Generated synthetic COCO dataset with full scenes\n",
                "\n",
                "**Output files:**\n",
                "- `synthetic_coco_dataset/train/` - Images with dice on backgrounds\n",
                "- `synthetic_coco_dataset/train/_annotations.coco.json` - COCO annotations\n",
                "- `gan_models/conditional_dcgan_dice.pth` - Trained GAN\n",
                "\n",
                "**Usage in other notebooks:**\n",
                "```python\n",
                "from src.dataset import DiceDetectionDataset\n",
                "\n",
                "synthetic_train = DiceDetectionDataset(\n",
                "    root_dir='synthetic_coco_dataset/train',\n",
                "    annotation_file='_annotations.coco.json',\n",
                "    split='train'\n",
                ")\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
