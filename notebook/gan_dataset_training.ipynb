{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dice Detection Training — GAN Augmented Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!sudo apt-get update && sudo apt-get install -y libavif-dev libheif-dev\n",
        "!pip install roboflow matplotlib seaborn tqdm numpy pillow\n",
        "\n",
        "# Clone and setup the project\n",
        "import os\n",
        "if not os.path.exists('Dice-Detection'):\n",
        "    !git clone https://github.com/Adr44mo/Dice-Detection.git\n",
        "os.chdir('Dice-Detection')\n",
        "!pip install -e .\n",
        "print('\\n✓ Setup complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Import custom modules\n",
        "from src.dataset import DiceDetectionDataset, collate_fn\n",
        "from src.model import get_fasterrcnn_model, save_model_checkpoint\n",
        "from src.training import train_one_epoch, evaluate, get_optimizer, get_lr_scheduler\n",
        "from src.metrics import evaluate_map, print_metrics\n",
        "from src.visualization import plot_training_history\n",
        "from src.aug.annotation_manager import AnnotationManager\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAINING CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 16\n",
        "NUM_WORKERS = 8\n",
        "NUM_EPOCHS = 5\n",
        "LEARNING_RATE = 0.005\n",
        "\n",
        "# Using the default Faster R-CNN loss\n",
        "\n",
        "# =============================================================================\n",
        "# ANNOTATION FILES  (GAN augmented dataset)\n",
        "# =============================================================================\n",
        "TRAIN_ANNOTATION = 'synthetic_coco_dataset/train/annotations/gan_train.coco.json'\n",
        "VAL_ANNOTATION = 'synthetic_coco_dataset/train/annotations/gan_val.coco.json'\n",
        "TEST_ANNOTATION = 'test_balanced.coco.json'\n",
        "\n",
        "# =============================================================================\n",
        "# DATASET PATHS\n",
        "# =============================================================================\n",
        "# GAN-generated images for train/val\n",
        "GAN_DATASET_BASE = os.path.join('.', 'synthetic_coco_dataset')\n",
        "TRAIN_PATH = os.path.join(GAN_DATASET_BASE, 'train', 'train')\n",
        "VAL_PATH = os.path.join(GAN_DATASET_BASE, 'val', 'train')\n",
        "\n",
        "# Test set: original Roboflow data (downloaded separately)\n",
        "# Update this path if your Roboflow dataset is at a different location\n",
        "ROBOFLOW_DATASET_PATH = 'dice-2'\n",
        "TEST_PATH = os.path.join(ROBOFLOW_DATASET_PATH, 'test')\n",
        "\n",
        "# Device\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# =============================================================================\n",
        "# PRINT SUMMARY\n",
        "# =============================================================================\n",
        "print('='*60)\n",
        "print('GAN DATASET TRAINING CONFIGURATION')\n",
        "print('='*60)\n",
        "print(f'\\n[Annotation Files]')\n",
        "print(f'  Train: {TRAIN_ANNOTATION}')\n",
        "print(f'  Val:   {VAL_ANNOTATION}')\n",
        "print(f'  Test:  {TEST_ANNOTATION}')\n",
        "print(f'\\n[Dataset Paths]')\n",
        "print(f'  Train images: {TRAIN_PATH}')\n",
        "print(f'  Val images:   {VAL_PATH}')\n",
        "print(f'  Test images:  {TEST_PATH}')\n",
        "print(f'\\n[Training]')\n",
        "print(f'  Batch size: {BATCH_SIZE}')\n",
        "print(f'  Epochs:     {NUM_EPOCHS}')\n",
        "print(f'  LR:         {LEARNING_RATE}')\n",
        "print(f'  Device:     {DEVICE}')\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Annotations & Prepare Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Annotation Manager\n",
        "anno_manager = AnnotationManager('./Annotations')\n",
        "\n",
        "# Load annotations\n",
        "train_anno = anno_manager.load_annotation_set(TRAIN_ANNOTATION)\n",
        "val_anno = anno_manager.load_annotation_set(VAL_ANNOTATION)\n",
        "test_anno = anno_manager.load_annotation_set(TEST_ANNOTATION)\n",
        "\n",
        "# Print stats\n",
        "for name, ann_file in [('Train', TRAIN_ANNOTATION), ('Val', VAL_ANNOTATION), ('Test', TEST_ANNOTATION)]:\n",
        "    stats = anno_manager.get_dataset_stats(ann_file)\n",
        "    print(f'{name}: {stats[\"num_images\"]} images, {stats[\"num_annotations\"]} annotations')\n",
        "\n",
        "# Write annotations to dataset directories\n",
        "import shutil\n",
        "for path in [TRAIN_PATH, VAL_PATH, TEST_PATH]:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "for path, anno in [(TRAIN_PATH, train_anno), (VAL_PATH, val_anno), (TEST_PATH, test_anno)]:\n",
        "    with open(os.path.join(path, '_annotations.coco.json'), 'w') as f:\n",
        "        json.dump(anno, f)\n",
        "\n",
        "print('\\n✓ Annotations copied to dataset directories')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create Datasets & Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets (no augmentation — GAN data is already augmented)\n",
        "train_dataset = DiceDetectionDataset(\n",
        "    root_dir=TRAIN_PATH,\n",
        "    annotation_file='_annotations.coco.json',\n",
        "    split='train'\n",
        " )\n",
        "\n",
        "val_dataset = DiceDetectionDataset(\n",
        "    root_dir=VAL_PATH,\n",
        "    annotation_file='_annotations.coco.json',\n",
        "    split='val'\n",
        " )\n",
        "\n",
        "# Test dataset\n",
        "has_test_set = os.path.exists(TEST_PATH) and os.path.exists(os.path.join(TEST_PATH, '_annotations.coco.json'))\n",
        "if has_test_set:\n",
        "    test_dataset = DiceDetectionDataset(\n",
        "        root_dir=TEST_PATH,\n",
        "        annotation_file='_annotations.coco.json',\n",
        "        split='test'\n",
        "    )\n",
        "else:\n",
        "    test_dataset = None\n",
        "\n",
        "print(f'Training dataset:   {len(train_dataset)} images')\n",
        "print(f'Validation dataset: {len(val_dataset)} images')\n",
        "if has_test_set:\n",
        "    print(f'Test dataset:       {len(test_dataset)} images')\n",
        "else:\n",
        "    print('Test dataset:       Not available (will use validation for evaluation)')\n",
        "print(f'Number of classes:  {train_dataset.num_classes}')\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    collate_fn=collate_fn\n",
        " )\n",
        "\n",
        "val_loader = data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    collate_fn=collate_fn\n",
        " )\n",
        "\n",
        "if has_test_set:\n",
        "    test_loader = data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "else:\n",
        "    test_loader = None\n",
        "\n",
        "print(f'\\nTraining batches:   {len(train_loader)}')\n",
        "print(f'Validation batches: {len(val_loader)}')\n",
        "if has_test_set:\n",
        "    print(f'Test batches:       {len(test_loader)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Faster R-CNN model\n",
        "model = get_fasterrcnn_model(\n",
        "    num_classes=train_dataset.num_classes,\n",
        "    pretrained=True,\n",
        "    trainable_backbone_layers=3\n",
        ")\n",
        "model.to(DEVICE)\n",
        "\n",
        "# Setup optimizer and scheduler\n",
        "optimizer = get_optimizer(model, lr=LEARNING_RATE)\n",
        "lr_scheduler = get_lr_scheduler(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "print(f'Model initialized on {DEVICE}')\n",
        "print('  Using default Faster R-CNN detection loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'learning_rate': []\n",
        "}\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "# Checkpoint directory\n",
        "CHECKPOINT_DIR = 'checkpoints_gan'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "config_str = 'gan'\n",
        "\n",
        "print('Starting training...')\n",
        "print(f'Checkpoint directory: {CHECKPOINT_DIR}\\n')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Train\n",
        "    train_metrics = train_one_epoch(\n",
        "        model, optimizer, train_loader, DEVICE, epoch + 1\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    val_metrics = evaluate(model, val_loader, DEVICE)\n",
        "\n",
        "    # Update learning rate\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # Record history\n",
        "    history['train_loss'].append(train_metrics['loss'])\n",
        "    history['val_loss'].append(val_metrics['val_loss'])\n",
        "    history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_metrics['loss']:.4f}\")\n",
        "    print(f\"  Val Loss: {val_metrics['val_loss']:.4f}\")\n",
        "    print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    print(f\"  Time: {train_metrics['time']:.2f}s\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_metrics['val_loss'] < best_val_loss:\n",
        "        best_val_loss = val_metrics['val_loss']\n",
        "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f'best_model_{config_str}.pth')\n",
        "        save_model_checkpoint(\n",
        "            model, optimizer, epoch + 1, val_metrics['val_loss'],\n",
        "            checkpoint_path,\n",
        "            additional_info={\n",
        "                'train_loss': train_metrics['loss'],\n",
        "                'config': config_str\n",
        "            }\n",
        "        )\n",
        "        print(f'  ✓ New best model saved!')\n",
        "\n",
        "    # Save latest checkpoint\n",
        "    latest_path = os.path.join(CHECKPOINT_DIR, f'latest_model_{config_str}.pth')\n",
        "    save_model_checkpoint(\n",
        "        model, optimizer, epoch + 1, val_metrics['val_loss'],\n",
        "        latest_path\n",
        "    )\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('Training completed!')\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Plot Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_training_history({\n",
        "    'Training Loss': history['train_loss'],\n",
        "    'Validation Loss': history['val_loss'],\n",
        "    'Learning Rate': history['learning_rate']\n",
        "})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
